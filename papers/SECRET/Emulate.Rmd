---
title: "Time series example"
author: "JS"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE, fig.height = 3)
```

Using the pulmonary model from the SECRET competition.

Load in design, data:

```{r, echo = FALSE, include=FALSE}
library(R.matlab)
library(ggplot2)
library(reshape2)
library(viridis)
library(cowplot)
library(fields)

setwd('~/Dropbox/UQ')
source('code/Gasp.R')
```


```{r}
design <- readRDS('data/designW1.rds')

t <- 512 # number of timepoints
v <- 3 # number of variables
n <- 100 # number of simulations

all_data <- array(0, dim = c(t,v,n))
for (i in 1:n){
  tmp <- readMat(paste0('data/output/flow', i, '.mat'))[[1]]
  all_data[,,i] <- tmp
}
```

Plot all data:

```{r}
plot_data <- data.frame(Time = 1:t,
                        Run = rep(1:n, each = t*v),
                        Output = c(all_data),
                        Type = rep(c('Flow1', 'Flow2', 'Pressure'), each = t))

plot_data2 <- melt(plot_data, id.vars = c('Time', 'Run', 'Type'))

ggplot(plot_data2, aes(x = Time, y = value, col = as.factor(Run))) +
  geom_line() +
  facet_wrap(vars(Type), nrow = 2, scales = 'free_y') +
  theme(legend.position = 'none')
```


## Emulate single output

Let's select timepoint 200 from Flow1 as our output. Creating dataframe, and plotting against 1 of the inputs:

```{r, fig.height=3}
design$ID <- NULL # don't need this column
tData <- data.frame(design[,1:4], y = all_data[200,1,])
head(tData)
ggplot(tData, aes(kMV, y)) + geom_point()
```

The inputs here are on very different scales. We should standardise these prior to modelling. Here, we have a fixed range for each input, so we use the min/max for each to scale these to [-1,1]:

```{r}
tData$kMV <- (tData$kMV - 9*10^4) / ((3*10^5 - 9*10^4)/2) - 1
tData$alpha <- (tData$alpha - 0.83) / ((0.89 - 0.83)/2) - 1
tData$lrrA <- (tData$lrrA - 20) / ((50 - 20)/2) - 1
tData$lrrV <- (tData$lrrV - 20) / ((50 - 20)/2) - 1
summary(tData)
```

We probably want to split into training/test sets at this point. We could do this manually, or we can handle this internally in the emulation code.

To do use this code, there's 1 more thing we need to do. From the notes in `Gasp.R`, the input data must have the structure [design, Noise, output]. This is because a noise term is used in the selection of a mean function:

```{r}
tData <- data.frame(tData[,1:4],
                    Noise = runif(nrow(tData), -1, 1),
                    y = tData$y)
```

Now we can run the code:

```{r}
em1 <- BuildGasp('y', tData)
summary(em1)
```

This has stored an rgasp emulator under `$em`, what mean function was used (here none, hence `NULL`), and also what the training data and validation data were. If you look at `Gasp.R` or the raw code for `BuildGasp`, you'll see there's an input `training_prop`, which will split the data into training and validation sets, with a default of 75\% used for training.

We can validate in several ways:

```{r}
par(mar = c(4,2,2,2));ValidateGasp(em1)
```

This function has several options. If you provide it with only an emulator, it will predict over the validation data stored in the `BuildGasp` object. You can alternatively provide it with a new dataset. You can also get it to plot the predictions against the inputs:

```{r}
par(mfrow = c(2,3), mar = c(4,2,2,2));ValidateGasp(em1, IndivPars = TRUE)
```

Alternatively, can do leave-one-out across the training data:

```{r}
par(mar = c(4,2,2,2));LeaveOneOut(em1)
```

This emulator seems reasonably good, but we could try something more complicated in the mean function. Could just do linear:

```{r}
em2 <- BuildGasp('y', tData, mean_fn = 'linear')
par(mfrow=c(1,2),mar = c(4,2,2,2));ValidateGasp(em2);LeaveOneOut(em2)
```

Or could fit something more general:

```{r, eval=FALSE}
em3 <- BuildGasp('y', tData, mean_fn = 'step')
```

```{r, include=FALSE}
em3 <- BuildGasp('y', tData, mean_fn = 'step')
```

```{r}
par(mfrow=c(1,2),mar = c(4,2,2,2));ValidateGasp(em3);LeaveOneOut(em3)
```

Here, we've fitted a more complicated mean surface initially - we have some new entries in our emulator:

```{r}
summary(em3)
```

`$lm` is a fitted linear regression object, giving more complex mean structure:

```{r}
summary(em3$lm$linModel)
```

`$active` is a list of which of the input variables are treated as active when fitting the GP to the residuals (essentially, in the mean fitting process we may find that some inputs are just noise, hence we don't include these when fitting the covariance):

```{r}
em3$active
```

By default, this mean function is not allowed more terms than 0.1*number of training points. However, this won't always work, and option `maxdf` allows flexibility in this specification (e.g., we may find we overfitted the data and want to limit the number of terms being added).


## Emulate full output

The above emulated a single output, but this code could be used to emulate $\ell$ outputs by looping over different outputs.

Alternatively, we could use dimension reduction to simplify this task. Let's emulate Flow1. First, we want to construct a basis. This is easy to do (this function has a lot more flexibility than is usually needed, e.g., can do weighted SVD for general matrices):

```{r}
DataBasis <- MakeDataBasis(all_data[,1,])
summary(DataBasis)
dim(DataBasis$tBasis)
dim(DataBasis$CentredField)
```

By default, this function subtracts the ensemble mean, and then calculates the SVD across the (centred) data. `$Q` and `$Lambda` are only stored if this function is used for weighted SVD, hence are not returned here (and indeed, not required in future calculations).

Plotting the leading few vectors:

```{r}
q <- 9
plot_basis <- data.frame(Time = rep(1:t, q),
                         Vector = rep(1:q, each = t),
                         Weight = c(DataBasis$tBasis[,1:q]))
ggplot(plot_basis, aes(Time, Weight)) +
  geom_line() +
  facet_wrap(vars(Vector))
```

Generally we truncate this basis for some small $q$ (such that a large enough amount of variability has been explained and/or such that any later vectors are just noise, with no signal from the parameters).

```{r}
q <- ExplainT(DataBasis, vtot = 0.95)
q
```

Here, 4 vectors explain 95\% of the variability in the data. In practice, we might want to try emulating the 5th, 6th etc. to see if these are predictable, but let's just use the 1st 4 for now.

Projecting the data onto the basis:

```{r}
Coeffs <- Project(data = DataBasis$CentredField, 
                  basis = DataBasis$tBasis[,1:q])
colnames(Coeffs)[1:q] <- paste("C",1:q,sep="")
summary(Coeffs)
```


```{r}
tDataC <- data.frame(tData[,1:4], # getting the scaled version from before
                     Noise = runif(n, -1, 1), 
                     Coeffs)
head(tDataC)
```

Now we can build emulators for these $q$ coefficients. We can do this simultaneously for all $q$ (as long as we're happy using the same assumptions for each). To make things consistent across each emulator, I'm going to define training/validation sets by hand, and then fit to the full training set (rather than allowing the function to do this split for me):

```{r}
set.seed(321)
inds <- sample(1:n, n)
train_inds <- inds[1:75]
val_inds <- inds[-c(1:75)]
train_data <- tDataC[train_inds,]
val_data <- tDataC[val_inds,]
```


```{r, eval = FALSE}
em_coeffs <- BasisEmulators(tDataC, q, mean_fn = 'step', maxdf = 5, training_prop = 1)
```

```{r, include=FALSE}
em_coeffs <- BasisEmulators(train_data, q, mean_fn = 'step', maxdf = 5, training_prop = 1)
```

Now in `ValidateGasp`, we need to provide the validation set (it's no longer internal to the `BasisEmulator` object):

```{r}
par(mfrow=c(2,2), mar=c(4,2,2,2))
ValidateGasp(em_coeffs[[1]], val_data)
ValidateGasp(em_coeffs[[2]], val_data)
ValidateGasp(em_coeffs[[3]], val_data)
ValidateGasp(em_coeffs[[4]], val_data)

par(mfrow=c(2,2), mar=c(4,2,2,2))
LeaveOneOut(em_coeffs[[1]]);LeaveOneOut(em_coeffs[[2]]);LeaveOneOut(em_coeffs[[3]]);LeaveOneOut(em_coeffs[[4]])
```


## Prediction

The validation plots above are performing prediction across the test data. In general, we can predict for any sets of inputs, for either a 1D emulator, or for a set of basis emulators. Doing so across a space-filling design in parameter space (here, we are still working in [-1,1]):

```{r}
ns <- 1000 # usually want more, but set low for speed
BigDesign <- 2*as.data.frame(randomLHS(ns, 4)) - 1 
colnames(BigDesign) <- colnames(design)[1:4]

Preds_1D <- PredictGasp(BigDesign, em3)
Preds_basis <- BasisPredGasp(BigDesign, em_coeffs)
```

These store slightly different things - in the 1D case, we get mean/sd/lower95/upper95 (the same as what `predict.rgasp` returns):

```{r}
summary(Preds_1D)
```

In the basis case, I only store `$Expectation` and `$Variance` (as this is all we need for history matching, and because predictive intervals can be derived from these, so don't want to store these if we have a lot of emulators):

```{r}
summary(Preds_basis)
```

Both of these objects are $ns \times q$ dimensional, where each row corresponds to an input we're predicting at, and each column corresponds to a basis vector:

```{r}
dim(Preds_basis$Expectation)
dim(Preds_basis$Variance)
```


## Reconstructing

From basis coefficients (whether given by projection, or predicted by an emulator), we can reconstruct an approximation of the original field.

Let's consider a run from the validation set.

```{r}
em_coeffs$V
```


